{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2fbac9-6621-4831-9ad4-2d7dcf7a23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## import\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db121916-121c-46e3-b40e-87ad332707a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 入力データの受け取り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bdc7f9e6-d625-44de-a778-7660c022e6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "テキストの文字数 : 3660\n",
      "最初の30文字 :  GMOグローバルサイン・ホールディングスCTO室のZulfa\n"
     ]
    }
   ],
   "source": [
    "with open('./train.txt','r',encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "\n",
    "print(\"テキストの文字数 :\", len(text))\n",
    "print(\"最初の30文字 : \",text[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0fbdc6-4e4f-42ab-9f73-f9d7b26aa88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "学習データで使っている文字数　：　 381\n",
      "トークン化した学習データ　：　 tensor([ 24,  28,  30, 134, 162, 165, 148, 161, 135, 130, 163, 164, 153, 165,\n",
      "        161, 146, 129, 163, 134, 138,  21,  35,  30, 230, 107,  40,  61,  53,\n",
      "         48,  43])\n"
     ]
    }
   ],
   "source": [
    "## 重複なしソート入力文字\n",
    "chars = sorted(list(set(text)))\n",
    "## 種類数\n",
    "char_size = len(chars)\n",
    "## 文字→数値、数値→文字の辞書\n",
    "char2int = { ch : i for i, ch in enumerate(chars) }\n",
    "int2char = { i : ch for i, ch in enumerate(chars) }\n",
    "## エンコード、デコードできる関数\n",
    "encode = lambda a: [char2int[b] for b in a ]\n",
    "decode = lambda a: ''.join([int2char[b] for b in a ])\n",
    "train_data = torch.tensor(encode(text), dtype=torch.long)\n",
    "\n",
    "print(\"学習データで使っている文字数　：　\", char_size)\n",
    "print(\"トークン化した学習データ　：　\", train_data[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f06e32cd-953e-4425-ac15-f85382282789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "埋め込みベクトルの次元数 :  3\n",
      "ベクトル表現 :  tensor([[ 1.3751, -0.1629, -1.1079],\n",
      "        [-0.2423, -1.2842,  0.6985],\n",
      "        [ 0.0325,  2.1288, -0.2214],\n",
      "        [-0.3532, -0.9633,  0.5138],\n",
      "        [ 0.8173,  0.3703,  0.0062]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## 1文字を何次元のベクトル表現にするか\n",
    "vector_size = 3\n",
    "embeddings = nn.Embedding(char_size, vector_size)\n",
    "encoded_words = torch.tensor(encode(\"前回の記事\"))\n",
    "embeddings_words  = embeddings(encoded_words)\n",
    "print(\"埋め込みベクトルの次元数 : \",vector_size)\n",
    "print(\"ベクトル表現 : \",embeddings_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21095d62-b6f5-41e5-8cb1-2f0c6f854363",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_Head(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_mbed, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        v = self.value(x)\n",
    "\n",
    "        wei = q @ k.transpose(-2,-1)* C ** -0.5\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf'))\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "\n",
    "        out = wei @ v\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250a794e-8aa4-4be3-9075-942f5943e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention_MultiHeads(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed, num_heads, head_size, block_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList((SelfAttention_Head(n_mbed, head_size, block_size) for _ in range(num_heads)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.cat([h(x) for h in self.heads], dim = -1)\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, n_mbed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(nn.Linear(n_mbed, n_mbed), nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67e353df-2305-476c-b0c8-0232a2ab0b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_mbed, char_size, block_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(char_size, n_mbed)\n",
    "        self.position_embedding = nn.Embedding(block_size, n_mbed)\n",
    "        self.selfattention_multiheads = SelfAttention_MultiHeads(n_mbed, 4, n_mbed//4, block_size)\n",
    "        self.feedforward = FeedForward(n_mbed)\n",
    "        self.linear = nn.Linear(n_mbed , char_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T= idx.shape\n",
    "        token_mbed = self.token_embedding(idx)\n",
    "        position_mbed = self.position_embedding(torch.arange(T))\n",
    "        x = token_mbed + position_mbed\n",
    "        x = self.selfattention_multiheads(x)\n",
    "        x = self.feedforward(x)\n",
    "        logits = self.linear(x)\n",
    "\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C =logits.shape\n",
    "            logits = logits.view(B*T,C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0f0492de-3407-4daa-923d-5d02bf2d2b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_heads = 4 # 同時に実行されるself-attentionの数\n",
    "block_size = 8 # トークンの埋め込むベクトルの次元数\n",
    "n_mbed = number_of_heads * block_size\n",
    "char_size = len(train_data)\n",
    "\n",
    "model = Model(n_mbed, char_size, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "afc57393-f860-40d5-a669-197ccf766b8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m logits, loss \u001b[38;5;241m=\u001b[39m model(\u001b[43mx\u001b[49m,y)\n\u001b[0;32m      2\u001b[0m idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m50\u001b[39m):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "logits, loss = model(x,y)\n",
    "idx = torch.zeros((1,1), dtype = torch.long)\n",
    "for _ in range(50):\n",
    "    idx_pred = idx[:, -block_size:]\n",
    "    logits , loss = model(idx_pred)\n",
    "    logits = logits[:,-1,:]\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    idx_next_pred = torch.multinomial(probs, num_samples=1)\n",
    "    idx = torch.cat((idx, idx_next_pred),dim = 1)\n",
    "\n",
    "predict = decode(idx[0].tolist())\n",
    "print(\"予測結果 : \", predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542a72e4-6943-4f50-aea6-8773191bede7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
